---
title: "Canonical tSNE Analysis"
author: "Ilaria Bonavita"
date: "18 January 2017"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 3D to 2D Example

We try the algorithm on a toy example.
First we generate two datasets. We consider here the problem of learning a 2-dimensional projections matrices from a 3-dimensional dataset.

```{r}
# dimension of the input features space
q <- 3
# dimension of the output transformed features
d <- 2
# Overal number of examples (train+test)
N <- 100
### Generate data 
MaxAngle <- 4*pi
MinRadius <-0.3
MaxRadius <- 8
sx <- 0.5
sy <- 0.5
set.seed(1244)
t <- seq(0, MaxAngle, length.out = N)
r <- seq(MinRadius, MaxRadius, length.out = N) + 2*runif(N)
#### generate X, the noise can be added!
set.seed(123)
X <- cbind(r*cos(t+0*rnorm(N)*0.05),r*sin(t+0*rnorm(N)*0.05),rnorm(N))
#### generate Y, the noise can be added!
Y <- cbind(t+0*rnorm(N)*1, 2*rnorm(N),rnorm(N))
```

We plot only the first two dimensions (the third dimension is basically noise)

```{r, echo=FALSE}
par(mfrow = c(1, 2))
  col_b2y <- colorRampPalette(c("blue", "cyan", "orange", "yellow"))
  plot(X, pch = 19, col = col_b2y(N), 
       main = "View of X", xlab = "X.1", ylab = "X.2", xlim = range(X[,1]*1.1), ylim = range(X[,2]*1.1))
  plot(Y, pch = 19, col = col_b2y(N), 
       main = "View of Y", xlab = "Y.1", ylab = "Y.2", xlim = range(Y[,1]*1.1), ylim = range(Y[,2]*1.3))
```

And we can plot the coordinates of X vs Y

```{r, echo=FALSE}
 par(mfrow = c(1, 2))
  col_b2y <- colorRampPalette(c("blue", "cyan", "orange", "yellow"))
  plot(X[,1], Y[,1] ,pch = 19, col = col_b2y(N), 
       main = "X1 vs Y1", xlab = "X.1", ylab = "Y.1", xlim = range(X[,1]*1.1), ylim = range(Y[,1]*1.1))
  plot(X[,2], Y[,2] ,pch = 19, col = col_b2y(N), 
       main = "X2 vs Y2", xlab = "X.2", ylab = "Y.2", xlim = range(X[,2]*1.1), ylim = range(Y[,2]*1.1))
```

We define a matrix of the distance between Xi and Xj (Yi and Yj) for all the possible combination of elements in X (Y)

```{r}
X.pairs <- combn(seq(1:nrow(X)),2)
Y.pairs <- combn(seq(1:nrow(Y)),2)

# Each row correspond to a pair (i,j), columns are the coordinates of the distance xi-xj
X.dist <- X[X.pairs[1,],] - X[X.pairs[2,],]
Y.dist <- Y[Y.pairs[1,],] - Y[Y.pairs[2,],]
```
Then we pass the distance matrices to a batch gradient descent algorithm to find the optimal projection matrices for dataset X and dataset Y

```
batch.grad.desc.fun(X.dist, Y.dist, N, dim.out=2,
                    gamma.list= c(0.06,0.05),
                    nexper = 1,
                    maxiter= 200,
                    wd.path= '/Users/ilaria_bonavita/My_project/tSNEcca',
                    ...
                    )
``` 

The function returns the two optimal matrices W.opt and Z.opt. We then map the 3-d points into the 2-d space 


```{r, echo=FALSE}
mat.path <- '/Users/ilaria_bonavita/My_project/tSNEcca/N100_in3_out2_rnd/exp_1gamma0.05/'
W.opt <- readRDS(paste0(mat.path,'Wout.RDS'))
Z.opt <- readRDS(paste0(mat.path,'Zout.RDS'))
```
```{r}
  XW <- W.opt%*%t(X)
  YZ <- Z.opt%*%t(Y)
```

```{r, echo=FALSE}
par(mfrow = c(1, 2))
  col_b2y <- colorRampPalette(c("blue", "cyan", "orange", "yellow"))
  plot(t(XW), pch = 19, col = col_b2y(N), 
       main = "View of X", xlab = "X.1", ylab = "X.2", xlim = range(XW[1,]*1.1), ylim = range(XW[2,]*1.1))
  plot(t(YZ), pch = 19, col = col_b2y(N), 
       main = "View of Y", xlab = "Y.1", ylab = "Y.2", xlim = range(YZ[1,]*1.1), ylim = range(YZ[2,]*1.3))

```

We can also plot the coordinates of X vs Y to see what's going on

```{r, echo=FALSE}
par(mfrow = c(1, 2))
  col_b2y <- colorRampPalette(c("blue", "cyan", "orange", "yellow"))
  plot(XW[1,], YZ[1,] ,pch = 19, col = col_b2y(N), 
       main = "View of X1 vs Y1", xlab = "X.1", ylab = "Y.1", xlim = range(XW[1,]*1.1), ylim = range(YZ[1,]*1.1))
  plot(XW[2,],YZ[2,], pch = 19, col = col_b2y(N), 
       main = "View of X2 vs Y2", xlab = "X.2", ylab = "Y.2", xlim = range(XW[2,]*1.1), ylim = range(YZ[2,]*1.3))
```

## Comparison with other Canonical Correlation Analysis methods

If we apply a simple CCA, we obtain the following results

```{r, echo=F,message=F,warning=F}
# Perform CCA

cca_res  <- cancor(X,Y)

XWcca <- X %*% cca_res$xcoef
YZcca <- Y %*% cca_res$ycoef

par(mfrow = c(1, 2))
col_b2y <- colorRampPalette(c("blue", "cyan", "orange", "yellow"))
plot(XWcca, pch = 19, col = col_b2y(N), 
     main = "View of X", xlab = "X.1", ylab = "X.2", xlim = range(XWcca[,1]*1.1), ylim = range(XWcca[,2]*1.1))
plot(YZcca, pch = 19, col = col_b2y(N), 
     main = "View of Y", xlab = "Y.1", ylab = "Y.2", xlim = range(YZcca[,1]*1.1), ylim = range(YZcca[,2]*1.3))

par(mfrow = c(1, 2))
col_b2y <- colorRampPalette(c("blue", "cyan", "orange", "yellow"))
plot(XWcca[,1], YZcca[,1] ,pch = 19, col = col_b2y(N), 
     main = "View of X1 vs Y1", xlab = "X.1", ylab = "Y.1", xlim = range(XWcca[,1]*1.1), ylim = range(YZcca[,1]*1.1))
plot(XWcca[,2],YZcca[,2], pch = 19, col = col_b2y(N), 
     main = "View of X2 vs Y2", xlab = "X.2", ylab = "Y.2", xlim = range(XWcca[,2]*1.1), ylim = range(YZcca[,2]*1.3))

```

This is what we obtain if we transform the input dataset with Nonparametric CCA (NCCA) with Gaussian KDE (1d)

```{r, echo=FALSE,message=FALSE, warning=FALSE}

require(FNN)
require(Matrix)
require(irlba)
source('./ncca.R')
ncca_res <- ncca(X,Y, d = 2, hx = 0.75, hy = 0.75, nx=5, ny=5, verbose = F)

X_proj_paired <- ncca_res$X_new
Y_proj_paired <- ncca_res$Y_new

par(mfrow = c(1, 2))
col_b2y <- colorRampPalette(c("blue", "cyan", "orange", "yellow"))
plot(X_proj_paired, pch = 19, col = col_b2y(N), 
     main = "View of X", xlab = "X.1", ylab = "X.2", xlim = range(X_proj_paired[,1]*1.1), ylim = range(X_proj_paired[,2]*1.1))
plot(Y_proj_paired, pch = 19, col = col_b2y(N), 
     main = "View of Y", xlab = "Y.1", ylab = "Y.2", xlim = range(Y_proj_paired[,1]*1.1), ylim = range(Y_proj_paired[,2]*1.3))

par(mfrow = c(1, 2))
col_b2y <- colorRampPalette(c("blue", "cyan", "orange", "yellow"))
plot(X_proj_paired[,1], Y_proj_paired[,1] ,pch = 19, col = col_b2y(N), 
     main = "Projections X1 vs Y1", xlab = "X.1", ylab = "Y.1", xlim = range(X_proj_paired[,1]*1.1), ylim = range(Y_proj_paired[,1]*1.1))
plot(X_proj_paired[,2],Y_proj_paired[,2], pch = 19, col = col_b2y(N), 
     main = "Projections X2 vs Y2", xlab = "X.2", ylab = "Y.2", xlim = range(X_proj_paired[,2]*1.1), ylim = range(Y_proj_paired[,2]*1.3))

```

We also applied Kernel CCA (KCCA). The plot shows the dataset X against Y projected in the kernel latent space

```{r, echo=FALSE,message=FALSE, warning=FALSE}
# Perform KCCA
library(kernlab)
library(geigen)
source('./kerncca.R')

kerncca_res <- kerncca(X,Y)

plot(kerncca_res$latent1,kerncca_res$latent2, pch = 19, col = col_b2y(N), 
     main = "Projections X vs Y", xlab = "X.2", ylab = "Y.2", xlim = range(kerncca_res$latent1*1.1), ylim = range(kerncca_res$latent2*1.3))

```





